The following are some of the things that I have planned for this system.
-------------------------------------------------------------------------
A simple distributed system that should be able to run on arbitrary number of machines and be able to do simple operations (like multiplication of matrices).

Each slave machine is served with both the data and the "processor" to work with.
The slave machine has a client running which receives work units from its master and send the master a "message" when its done processing the work unit.

The master schedules the jobs (input by a user e.g.) and does load-balancing among its slaves. When all the work units are complete for a particular job,
the master "reduces" them to the required result.

No failure detection and work unit transfer are planned yet.

--[ Implementation
Each physical machine in a cluster is a "node".
- The slaves need a <hostname>-slave-config.json file.

The node can be run in one of the two modes:
  - Master: This node will function as a master assigning work to
    its slaves. The master is given "jobs" by the user and the jobs are
    split into tasks to be assigned to slaves.
  - Slave only: This node will function as a slave accepting work from the
    master, completing it, and sending the results back to master (or some other
    arrangement like storing the results on shared storage).

------[ Configuration files
In the future, we will have a configuration server (or just a specialized
Master who just configures the Slaves).

For now, each slave will need it's own config file.
- <hostname>-slave-config.json: The configuration used by the Node if the Node is a Slave
Node.
The essential parts of this configuration file are:
  - The root element is a dictionary with the key 'masters' present.
  - The value of this key is a list of masters.
  - Each element in this list is a dictionary with two keys in it: the
    'master-combine' and the 'master-assign'.
  - The values for each of these keys are dictionaries with the keys 'ip'
    and 'name'.

TODO: The above configuration can also be done using command line arguments.

------[ Serialization
The serialization scheme took some time to "implement in my head". The reason
being that I had to look carefully to see if it was possible to use some library
for python to serialize the objects. In particular, I need to serialize a task
unit and send it to a slave (from the master node). Now the complication here is
that the libraries provided by Python (e.g. pickle) can only serialize an
object to its "representation" and such libraries assume that the receiving end
will already have an idea of what this object is. In other words, they serialize
only the information about the metadata of a particular instance of an object.
For example if there as an object with a method inside it, then it cannot be
serialized by pickle.

So here's the method I came up with to "truly" serialize all the functionality
of an object assuming we already know the structure of the object:
First, I extract the code for the methods that I want to serialize from the
object like so:
<code>
import inspect

import taskunit

# Set the data and the processor.
tu = taskunit.TaskUnit("hello", lambda s: print(s))

method = tu.processor
# Get the source of the code as a string
method_code = inspect.getsource(method)
</code>

This gives us the source code of the required method.
Then I get all the other attributes of interest from the object and dump them
as a dictionary (including the data).
Then I construct a new dictionary with the following format:
<code>
{
  'processor' : 'def processor(x):\n    print(x * x)',
  'data': 4,
  'state': 'DEFINED'
   ...,
   ...,
   ...
}
</code>

This dictionary is then dumped as a JSON string and sent to the Slave (maybe
after compression). The slave then reads the JSON into Python lists/dictionaries
and extracts the required strings/values from the lists/dictionaries.
In particular, it extracts the processor's code. After doing so, it
"reconstructs" the TaskUnit object like so:

<code>
processor_string = decoded_json['processor']
data = decoded_json['data']
state_string = decoded_json['state']

exec(processor_string)
reconstructed_task_unit = TaskUnit(data_string, processor)
reconstructed_task_unit.state = state_string
</code>
The important part in this code above is the exec(...) call which basically
executes the code in the processor_string variable and defines the method
"processor" in the local scope. Then all we have to do is to create a new TaskUnit
and provide it with this processor and the data and that's basically it. If
there are other attributes that we also need to serialize, then we can also send
those through the JSON string over the network and decode on the Slave's end in
the same way as above.
